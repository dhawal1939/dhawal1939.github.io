<!DOCTYPE html>
<html lang="en">

  <!-- Head -->
  <head>    <!-- Metadata, OpenGraph and Schema.org -->
    

    <!-- Standard metadata -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Dhawal Sirikonda | Publications</title>
    <meta name="author" content="Dhawal  Sirikonda" />
    <meta name="description" content="
# A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design.
" />


    <!-- Bootstrap & MDB -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous" />

    <!-- Fonts & Icons -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous">
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

    <!-- Code Syntax Highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="none" id="highlight_theme_light" />

    <!-- Styles -->
    
    <link rel="shortcut icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>Ⓓ</text></svg>">
    
    <link rel="stylesheet" href="/assets/css/main.css">
    <link rel="canonical" href="https://dhawal1939.github.io/publications/">
    
    <!-- Dark Mode -->
    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/vs.css" media="none" id="highlight_theme_dark" />

    <script src="/assets/js/theme.js"></script>
    <script src="/assets/js/dark_mode.js"></script>
    

  </head>

  <!-- Body -->
  <body class="fixed-top-nav sticky-bottom-footer">

    <!-- Header -->
    <header>

      <!-- Nav Bar -->
      <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
        <div class="container">
          <a class="navbar-brand title font-weight-lighter" href="https://dhawal1939.github.io/">Dhawal Sirikonda</a>
          <!-- Navbar Toggle -->
          <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar top-bar"></span>
            <span class="icon-bar middle-bar"></span>
            <span class="icon-bar bottom-bar"></span>
          </button>

          <div class="collapse navbar-collapse text-right" id="navbarNav">
            <ul class="navbar-nav ml-auto flex-nowrap">

              <!-- About -->
              <li class="nav-item ">
                <a class="nav-link" href="/">About</a>
              </li>
              
              <!-- Blog -->
              <li class="nav-item ">
                <a class="nav-link" href="/blog/">blog</a>
              </li>

              <!-- Other pages -->
              <li class="nav-item active">
                <a class="nav-link" href="/publications/">Publications<span class="sr-only">(current)</span></a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/projects/">Research Projects</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/teaching/">Teaching</a>
              </li>

              <!-- Toogle theme mode -->
              <div class="toggle-container">
                <a id="light-toggle">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
                </a>
              </div>
            </ul>
          </div>
        </div>
      </nav>
    </header>

    <!-- Content -->
    <div class="container mt-5">
      <!-- page.html -->
        <div class="post">

          <header class="post-header">
            <h1 class="post-title">Publications</h1>
            <p class="post-description"></p>
          </header>

          <article>
            <!-- _pages/publications.md -->
<div class="publications">
  <h2 class="year">2023</h2>
  <ol class="bibliography"><li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge">ISRF</abbr></div>

        <!-- Entry bib key -->
        <div id="isrfgoel2023" class="col-sm-8">
        
        <!-- Teaser Image -->
          <!-- <div class="col-sm-2"></div> -->
          <!-- Title -->
          <div class="title">Interactive Segmentation of Radiance Fields</div>
          <!-- Conference -->
          <div class="Conf">
<b>Venue:</b> <i>CVPR ’23</i>
</div>
          <brea></brea>
          <!-- Author -->
          <div class="author">Rahul Goel*, Dhawal Sirikonda*, <a href="https://sophont01.github.io/" target="_blank" rel="noopener noreferrer">Saurabh Saini</a>, and P. J. Narayanan
          </div>
        

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em> 2023
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="https://arxiv.org/abs/2212.13545" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Author Version</a>
            <a href="https://rahul-goel.github.io/isrf/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Website</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Radiance Fields (RF) are popular to represent casually-captured scenes for new view generation and have been used for applications beyond it. Understanding and manipulating scenes represented as RFs have to naturally follow to facilitate mixed reality on personal spaces. Semantic segmentation of objects in the 3D scene is an important step for that. Prior segmentation efforts using feature distillation show promise but don’t scale to complex objects with diverse appearance. We present a framework to interactively segment objects with fine structure. Nearest neighbor feature matching identifies high-confidence regions of the objects using distilled features. Bilateral filtering in a joint spatio-semantic space grows the region to recover accurate segmentation. We show state-of-the-art results of segmenting objects from RFs and compositing them to another scene, changing appearance, etc., moving closer to rich scene manipulation and understanding.</p>
          </div>
        </div>
      </div>
</li></ol>

  <h2 class="year">2022</h2>
  <ol class="bibliography">
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge">PRTonAnySurf</abbr></div>

        <!-- Entry bib key -->
        <div id="dhawal2022prtsdf" class="col-sm-8">
        
        <!-- Teaser Image -->
          <!-- <div class="col-sm-2"></div> -->
          <!-- Title -->
          <div class="title">Real-Time Rendering of Arbitrary Surface Geometries using Learnt Transfer</div>
          <!-- Conference -->
          <div class="Conf">
<b>Venue:</b> <i>ICVGIP ’22</i>
</div>
          <brea></brea>
          <!-- Author -->
          <div class="author">
                <em>Sirikonda Dhawal</em>, <a href="https://aakashkt.github.io" target="_blank" rel="noopener noreferrer">Aakash KT</a>, and <a href="https://scholar.google.com/citations?user=3HKjt_IAAAAJ" target="_blank" rel="noopener noreferrer">P.J. Narayanan</a>
                  
          </div>
        

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>In Proceedings of the Thirteenth Indian Conference on Computer Vision, Graphics and Image Processing</em> 2022
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a><a href="https://doi.org/10.1145/3571600.3571640" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Publisher Version</a>
            <a href="https://dhawal1939.github.io/projects/sh_rendering/#learnttransfer" class="btn btn-sm z-depth-0" role="button">Website</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Precomputed Radiance Transfer (PRT) is widely used for real-time photorealistic effects. PRT disentangles the rendering equation into transfer and  lighting, enabling their precomputation. Transfer accounts for the cosine-weighted visibility of points in the scene while lighting for emitted radiance from the environment. Prior art stored precomputed transfer in a tabulated manner, either in vertex or texture space. These values are fetched with interpolation at each point for shading. Vertex space methods require densely tessellated mesh vertices for high quality images. Texture space methods require non-overlapping and area-preserving UV mapping to be available. They also require a high-resolution texture to avoid rendering artifacts. In this paper, we propose a compact transfer representation that is learnt directly on scene geometry points. Specifically, we train a small multi-layer perceptron (MLP) to predict the transfer at sampled surface points. Our approach is most beneficial where inherent mesh storage structure and natural UV mapping are not available, such as Implicit Surfaces as it learns the transfer values directly on the surface. We demonstrate real-time, photorealistic renderings of diffuse and glossy materials on SDF geometries with PRT using our approach.</p>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge">PRTonSDF</abbr></div>

        <!-- Entry bib key -->
        <div id="dhawal2022prtsdg" class="col-sm-8">
        
        <!-- Teaser Image -->
          <!-- <div class="col-sm-2"></div> -->
          <!-- Title -->
          <div class="title">Learnt Transfer for Surface Geometries</div>
          <!-- Conference -->
          <div class="Conf">
<b>Venue:</b> <i>High Performance Graphics (HPG) 2022-Poster</i>
</div>
          <brea></brea>
          <!-- Author -->
          <div class="author">
                <em>Sirikonda Dhawal</em>, <a href="https://aakashkt.github.io" target="_blank" rel="noopener noreferrer">Aakash KT</a>, and <a href="https://scholar.google.com/citations?user=3HKjt_IAAAAJ" target="_blank" rel="noopener noreferrer">P.J. Narayanan</a>
                  
          </div>
        

          <!-- Journal/Book title and date -->
          <div class="periodical">
            2022
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
<a href="https://www.highperformancegraphics.org/posters22/HPG2022_Poster7_Learnt_Transfer_for_Surface_Geometries.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Publisher Version</a>
            <a href="https://dhawal1939.github.io/projects/sh_rendering/#learnttransfer" class="btn btn-sm z-depth-0" role="button">Website</a>
          </div>

          
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge">StyleTRF</abbr></div>

        <!-- Entry bib key -->
        <div id="goel2022styletrf" class="col-sm-8">
        
        <!-- Teaser Image -->
          <!-- <div class="col-sm-2"></div> -->
          <!-- Title -->
          <div class="title">StyleTRF: Stylizing Tensorial Radiance Fields</div>
          <!-- Conference -->
          <div class="Conf">
<b>Venue:</b> <i>ICVGIP ’22</i>
</div>
          <brea></brea>
          <!-- Author -->
          <div class="author">
<a href="https://rahulgoel.xyz" target="_blank" rel="noopener noreferrer">Rahul Goel</a>, 
                <em>Sirikonda Dhawal</em>, <a href="https://sophont01.github.io/" target="_blank" rel="noopener noreferrer">Saurabh Saini</a>, and <a href="https://scholar.google.com/citations?user=3HKjt_IAAAAJ" target="_blank" rel="noopener noreferrer">P.J. Narayanan</a>
                  
          </div>
        

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>In Proceedings of the Thirteenth Indian Conference on Computer Vision, Graphics and Image Processing</em> 2022
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a><a href="https://doi.org/10.1145/3571600.3571643" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Publisher Version</a>
            <a href="https://rahul-goel.github.io/StyleTRF/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Website</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Stylized view generation of scenes captured casually using a camera has received much attention recently. The geometry and appearance of the scene are typically captured as neural point sets or neural radiance fields in the previous work. An image stylization method is used to stylize the captured appearance by training its network jointly or iteratively with the structure capture network. The state-of-the-art SNeRF method trains the NeRF and stylization network in an alternating manner. These methods have high training time and require joint optimization. In this work, we present StyleTRF, a compact, quick-to-optimize strategy for stylized view generation using TensoRF. The appearance part is fine-tuned using sparse stylized priors of a few views rendered using the TensoRF representation for a few iterations. Our method thus effectively decouples style-adaption from view capture and is much faster than the previous methods. We show state-of-the-art results on several scenes used for this purpose.</p>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge">PRTT</abbr></div>

        <!-- Entry bib key -->
        <div id="https://doi.org/10.48550/arxiv.2203.12399" class="col-sm-8">
        
        <!-- Teaser Image -->
          <!-- <div class="col-sm-2"><img class="teaser" src="/assets/img/paper_teasers/2203_12399.jpg" width="240px" height="200px"></div> -->
          <!-- Title -->
          <div class="title">PRTT: Precomputed Radiance Transfer Textures</div>
          <!-- Conference -->
          <div class="Conf">
<b>Venue:</b> <i>Technical Report</i>
</div>
          <brea></brea>
          <!-- Author -->
          <div class="author">
                <em>Sirikonda Dhawal</em>, <a href="https://aakashkt.github.io" target="_blank" rel="noopener noreferrer">Aakash KT</a>, and <a href="https://scholar.google.com/citations?user=3HKjt_IAAAAJ" target="_blank" rel="noopener noreferrer">P.J. Narayanan</a>
                  
          </div>
        

          <!-- Journal/Book title and date -->
          <div class="periodical">
            2022
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="https://arxiv.org/abs/2203.12399" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Author Version</a>
            <a href="https://dhawal1939.github.io/projects/sh_rendering/#uvtransfer" class="btn btn-sm z-depth-0" role="button">Website</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Precomputed Radiance Transfer (PRT) can achieve high quality renders of glossy materials at real-time framerates. PRT involves precomputing a k-dimensional transfer vector of Spherical Harmonic (SH) coefficients at specific points for a scene. Most prior art precomputes transfer at vertices of the mesh and interpolates color for interior points. They require finer mesh tessellations for high quality renderings. In this paper, we explore and present the use of textures for storing transfer. Using transfer textures decouples mesh resolution from transfer storage and sampling which is useful especially for glossy renders. We further demonstrate glossy inter-reflections by precomputing additional textures. We thoroughly discuss practical aspects of transfer textures and analyze their performance in real-time rendering applications. We show equivalent or higher render quality and FPS and demonstrate results on several challenging scenes.</p>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge">TransferTextures</abbr></div>

        <!-- Entry bib key -->
        <div id="10.2312:egp.20221012" class="col-sm-8">
        
        <!-- Teaser Image -->
          <!-- <div class="col-sm-2"></div> -->
          <!-- Title -->
          <div class="title">Transfer Textures for Fast Precomputed Radiance Transfer</div>
          <!-- Conference -->
          <div class="Conf">
<b>Venue:</b> <i>EuroGraphics ’22</i>
</div>
          <brea></brea>
          <!-- Author -->
          <div class="author">
                <em>Sirikonda Dhawal</em>, Aakash Kt, and P. J. Narayanan
          </div>
        

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>In Eurographics 2022 - Posters</em> 2022
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
<a href="https://doi.org/10.2312/egp.20221012" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Publisher Version</a>
          </div>

          
        </div>
      </div>
</li>
</ol>

  <h2 class="year">2021</h2>
  <ol class="bibliography">
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge">NeuralViewSyn</abbr></div>

        <!-- Entry bib key -->
        <div id="10.1145/3490035.3490299" class="col-sm-8">
        
        <!-- Teaser Image -->
          <!-- <div class="col-sm-2"></div> -->
          <!-- Title -->
          <div class="title">Neural View Synthesis with Appearance Editing from Unstructured Images</div>
          <!-- Conference -->
          <div class="Conf">
<b>Venue:</b> <i>ICVGIP ’21</i>
</div>
          <brea></brea>
          <!-- Author -->
          <div class="author">
<a href="https://darthgera123.github.io" target="_blank" rel="noopener noreferrer">Pulkit Gera</a>, Aakash K T, Dhawal Sirikonda, and P. J. Narayanan
          </div>
        

          <!-- Journal/Book title and date -->
          <div class="periodical">
            <em>In Proceedings of the Twelfth Indian Conference on Computer Vision, Graphics and Image Processing</em> 2021
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a><a href="https://doi.org/10.1145/3490035.3490299" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Publisher Version</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>We present a neural rendering framework for simultaneous view synthesis and appearance editing of a scene with known environmental illumination captured using a mobile camera. Existing approaches either achieve view synthesis alone or view synthesis along with relighting, without control over the scene’s appearance. Our approach explicitly disentangles the appearance and learns a lighting representation that is independent of it. Specifically, we jointly learn the scene appearance and a lighting-only representation of the scene. Such disentanglement allows our approach to generalize to arbitrary changes in appearance while performing view synthesis. We show results of editing the appearance of real scenes in interesting and non-trivial ways. The performance of our view synthesis approach is on par with state-of-the-art approaches on both real and synthetic data.</p>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge">AppearaceEdit</abbr></div>

        <!-- Entry bib key -->
        <div id="https://doi.org/10.48550/arxiv.2110.07674" class="col-sm-8">
        
        <!-- Teaser Image -->
          <!-- <div class="col-sm-2"><img class="teaser" src="/assets/img/paper_teasers/2110_07674.jpg" width="240px" height="200px"></div> -->
          <!-- Title -->
          <div class="title">Appearance Editing with Free-viewpoint Neural Rendering</div>
          <!-- Conference -->
          <div class="Conf">
<b>Venue:</b> <i>Technical Report</i>
</div>
          <brea></brea>
          <!-- Author -->
          <div class="author">
<a href="https://darthgera123.github.io" target="_blank" rel="noopener noreferrer">Pulkit Gera</a>, <a href="https://aakashkt.github.io" target="_blank" rel="noopener noreferrer">Aakash KT</a>, 
                <em>Sirikonda Dhawal</em>, <a href="https://researchweb.iiit.ac.in/~parikshit.sakurikar" target="_blank" rel="noopener noreferrer">Parikshit Sakurikar</a>, and <a href="https://scholar.google.com/citations?user=3HKjt_IAAAAJ" target="_blank" rel="noopener noreferrer">P.J. Narayanan</a>
                  
          </div>
        

          <!-- Journal/Book title and date -->
          <div class="periodical">
            2021
          </div>
        
          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="https://arxiv.org/abs/2110.07674" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Author Version</a>
            <a href="https://darthgera123.github.io/appearance-editing/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Website</a>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>We present a neural rendering framework for simultaneous view synthesis and appearance editing of a scene from multi-view images captured under known environment illumination. Existing approaches either achieve view synthesis alone or view synthesis along with relighting, without direct control over the scene’s appearance. Our approach explicitly disentangles the appearance and learns a lighting representation that is independent of it. Specifically, we independently estimate the BRDF and use it to learn a lighting-only representation of the scene. Such disentanglement allows our approach to generalize to arbitrary changes in appearance while performing view synthesis. We show results of editing the appearance of a real scene, demonstrating that our approach produces plausible appearance editing. The performance of our view synthesis approach is demonstrated to be at par with state-of-the-art approaches on both real and synthetic data.</p>
          </div>
        </div>
      </div>
</li>
</ol>


</div>

          </article>

        </div>

    </div>

    <!-- Footer -->    <footer class="sticky-bottom mt-5">
      <div class="container">
        © Copyright 2023 Dhawal  Sirikonda. 
      </div>
    </footer>

    <!-- JavaScripts -->
    <!-- jQuery -->
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

    <!-- Bootsrap & MDB scripts -->
  <script src="https://cdn.jsdelivr.net/npm/@popperjs/core@2.11.2/dist/umd/popper.min.js" integrity="sha256-l/1pMF/+J4TThfgARS6KwWrk/egwuVvhRzfLAMQ6Ds4=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.min.js" integrity="sha256-SyTu6CwrfOhaznYZPoolVw2rxoY7lKYKQvqbtqN93HI=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

    <!-- Masonry & imagesLoaded -->
  <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
  <script defer src="/assets/js/masonry.js" type="text/javascript"></script>
    
  <!-- Medium Zoom JS -->
  <script src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script>
  <script src="/assets/js/zoom.js"></script><!-- Load Common JS -->
  <script src="/assets/js/common.js"></script>

    <!-- MathJax -->
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        tags: 'ams'
      }
    };
  </script>
  <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
  <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

    
  </body>
</html>

